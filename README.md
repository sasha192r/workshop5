# Машинное обучение в Unity — ML-Agent
Отчет по лабораторной работе #4 выполнил(а):
- Маврин Александр Андреевич
- НМТ-232513

| Задание | Выполнение | Баллы |
| ------ | ------ | ------ |
| Задание 1 | * | 60 |
| Задание 2 | * | 20 |
| Задание 3 | * | 20 |

Работу проверили:
- к.т.н., доцент Денисов Д.В.
- к.э.н., доцент Панов М.А.
- ст. преп., Фадеев В.О.

[![N|Solid](https://cldup.com/dTxpPi9lDf.thumb.png)](https://nodesource.com/products/nsolid)

[![Build Status](https://travis-ci.org/joemccann/dillinger.svg?branch=master)](https://travis-ci.org/joemccann/dillinger)

Структура отчета

- Данные о работе: название работы, фио, группа, выполненные задания.
- Цель работы.
- Задание 1.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 2.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Задание 3.
- Код реализации выполнения задания. Визуализация результатов выполнения (если применимо).
- Выводы.

## Цель работы
Познакомиться с программными средствами для создания системы машинного обучения и ее интеграции в Unity.

## Задание 1. 
### Найдите внутри C# скрипта "коэффициент корреляции" и сделать выводы о том, как он влияет на обучение модели.
Параметр будет отвечать за то, на каком расстоянии от цели агент будет считаться успешным. Чем коэффициент больше, тем менее точно модель будет достигать цель, но и скорость обучения тем самым будет возрастать.  
Также это работает в обратную сторону, чем меньше коэффициент, тем дольше и корректнее будет обучение.

```C#
float distanceToTarget = Vector3.Distance(this.transform.localPosition, Target.localPosition);

if(distanceToTarget < 1.42f)
{
    SetReward(1.0f);
    EndEpisode();
}
```

## Задание 2
### Изменить параметры файла yaml-агента и определить какие параметры и как влияют на обучение модели. Привести описание не менее трех параметров.

1. `num_epoch` определяет количество проходов через полный набор данных для обучения модели. Меньшее количество эпох: обучение происходит быстрее, но модель может не успеть выучить достаточные закономерности из данных. Это может привести к недообучению.
Большее количество эпох: улучшает точность модели за счёт более глубокой проработки данных, но увеличивает время обучения. Слишком большое значение может привести к переобучению, когда модель начинает плохо обобщать данные.

2. `learning_rate` (скорость обучения) регулирует величину изменений, которые модель вносит в свои параметры после каждой итерации обучения. Низкое значение: обеспечивает стабильное обучение, но замедляет процесс достижения оптимального результата, так как шаги изменений очень малы.
Высокое значение: ускоряет процесс обучения, однако может привести к "перепрыгиванию" оптимальных значений и сделать обучение нестабильным.

3. `epsilon` используется для предотвращения деления на ноль или слишком малые значения в численных вычислениях (например, при обновлении весов или использовании градиентных методов). Маленькие значения: обеспечивают более точные вычисления, что может придать стабильность обучению, но процесс адаптации становится медленнее.
Большие значения: ускоряют адаптацию, однако могут вносить нежелательные отклонения и сделать процесс менее устойчивым.

## Задание 3
### Приведите примеры, для каких игровых задачи и ситуаций могут использоваться примеры 1 и 2 с ML-Agent’ом. В каких случаях проще использовать ML-агент, а не писать программную реализацию решения?

### Пример 1: Ищущий агент
- В гонках агент должен искать оптимальный путь к финишной черте, избегая препятствий и взаимодействуя с другими агентами.

#### Почему использовать ML-агента:
  
  - Динамическая среда: Если маршруты или поведение игрока непредсказуемы, использование ML-агента позволяет NPC адаптироваться к этим изменениям.
  - Например, игрок может создавать препятствия на пути NPC, или финишная черта может перемещаться.
  - Сложные решения: Агент может выбирать оптимальные пути, избегать ловушек и учитывать взаимодействие с другими объектами, что сложно прописать вручную.

### Пример 2: Агент доставки

- NPC должен доставить ресурсы от точки к точке, выбирая маршрут с учётом препятствий и других участников.

### Почему использовать ML-агента:
  - Изменяющаяся карта: Если расположение точки, зданий или препятствий меняется в каждой игре, агент может обучиться искать наиболее эффективный маршрут.
  - Взаимодействие: Если несколько NPC конкурируют за ресурсы или координируют свои действия, агент может обучиться учитывать их поведение.

### Примеры ситуаций, где предпочтительнее ML-агенты:
Сложная и изменчивая среда:
- Игровая карта или препятствия создаются случайно каждый раз, когда игрок начинает новую сессию.
- Требуется реакция NPC на действия игрока в реальном времени.
Многомерные задачи:
- Агент должен одновременно доставлять ресурсы, избегать врагов и оптимизировать своё время.
Трудность предсказания:
- Игроки имеют множество стратегий, и требуется, чтобы NPC адаптировался к разным стилям игры.
Случаи для программной реализации:

### Чёткие алгоритмы: Например, использование жёстко заданных правил, как в играх с заранее определёнными условиями.
ML-агенты оправданы там, где требуется гибкость и адаптивность, которых сложно достичь классическими программными подходами.

## Выводы
1.  Изучены программные средства для создания систем машинного обучения и их интеграции в Unity. Это позволяет внедрять адаптивные механики поведения агентов в игровые проекты.
2.  Сделаны выводы о значении коэффициента корреляции в скриптах и его влиянии на процесс обучения модели. Корреляция между входными данными и результатами обучения является важным показателем для оценки эффективности модели.
3.  Разобраны параметры конфигурации YAML-файлов ML-агента, такие как num_epoch, learning_rate, epsilon, а также их влияние на обучение модели
4.  Приведены практические примеры использования ML-агентов:
- Пример 1: Ищущий агент.
- Пример 2: Агент, выполняющий задачи по доставке.
5.  Выделены случаи, когда проще использовать ML-агентов, а не программную реализацию:
- При сложной, динамично изменяющейся среде.
- При необходимости адаптивного поведения NPC.
- В задачах, где классическое программирование требует избыточного объёма ручного кода.
